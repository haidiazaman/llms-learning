{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f394f6-9a55-456c-a09a-f6264d8dea1d",
   "metadata": {},
   "source": [
    "# Build a Conversational RAG app with Custom PDF ingestion using Ollama-Langchain\n",
    "Goals:\n",
    "* use open-source LLM from Ollama for ChatCompletion\n",
    "* use open-source embedding model from HuggingFace for Embeddings for VectorStore\n",
    "* once done, convert to python script\n",
    "Document using: https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf\n",
    "* it has been manually split by chaps into ~20 separate pdfs\n",
    "\n",
    "* should also have a version whre you can pass links and then ingest using import requests, but will have less control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2542b50d-62d0-42d3-aa2b-d0bed1aaea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0ec9a6-ec2a-4773-b503-e3f539c6c6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap11.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap13.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap12.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/cover-content-page.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap16.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap17.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/index.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap15.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap14.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap8.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap9.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap1.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/references.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/author-index.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap2.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap3.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap7.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap6.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap4.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap5.pdf',\n",
       " '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap18.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_paths = glob.glob(\"/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/*.pdf\")\n",
    "files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "698e238f-61ff-4257-8a38-4b7cf8ccd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdfs into list\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_pdfs(file_paths):\n",
    "    \"\"\"\n",
    "    file_paths must end with .pdf\n",
    "    PyPDFLoader auto splits the pdf into pages, each page is 1 Document object\n",
    "\n",
    "    returns a dict of key: file_path and value: list of document objects\n",
    "    \"\"\"\n",
    "    documents_dict = {}   \n",
    "    for f in tqdm(file_paths):\n",
    "        loader = PyPDFLoader(file_path = f)\n",
    "        documents = loader.load()\n",
    "        documents_dict[f] = documents\n",
    "    return documents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "692a9765-518b-4624-925b-3c15435a65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:33<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "documents_dict = load_pdfs(file_paths=files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3939bab-9e61-4397-9f13-f346e521a441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 22)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dict) == len(files_paths), len(documents_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c16cb5e7-2f15-4df5-9d5c-4e00e9113a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap11.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap13.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap12.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/cover-content-page.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap16.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap17.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/index.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap15.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap14.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap8.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap9.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap1.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/references.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/author-index.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap2.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap3.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap7.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap6.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap4.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap5.pdf\n",
      "/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap18.pdf\n"
     ]
    }
   ],
   "source": [
    "# print all the keys\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for k in documents_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4aca7ec0-5652-49f9-b954-6de44f9a7ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dict['/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05d7df6c-9e2e-422b-a9b9-8048ddf8ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = documents_dict['/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ef14cbe-14a8-40b5-ba43-97c5dfeffc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0da13-59f6-4e83-b5a9-07dc1177daca",
   "metadata": {},
   "source": [
    "as you can see even though PyPDFLoader splits the pdf into different Document objects for each page, each Document object is still considered huge number of char\n",
    "- so there is still a need to split into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693b053-dd0f-41c8-8261-89a4af657f20",
   "metadata": {},
   "source": [
    "if you have a lot RAM, you can afford to split into smaller chunk sizes \n",
    "\n",
    "* small chunk size, will help the model with smaller context window when referencing the document\n",
    "* big chunk size ensures the text is too split up causing the text to lose its meaning\n",
    "\n",
    "example\n",
    "like for example, one chunk talks about intro to decision trees then another chunk is about random forest using bootstrapping of decision trees\n",
    "query: \"why is smaller decision trees better?\"\n",
    "Your RAG will rank both contexts highly, often giving unreliable results because your chunk is too small to capture the meaning. Your RAG might not answer the question because the question is in the extended portion of context that happens to be in another chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a8d3b-1f46-492c-bf9c-9f7bd4205e89",
   "metadata": {},
   "source": [
    "Nic Ang suggestion to choose the right number for chunk_size\n",
    "\n",
    "- use traditional NLP techniques like BOW or sth, to calculate the average number of characters, average number of words, pdf page\n",
    "then you calculate the average number of characters in each word for each page, aggregate across all documents, then you get your # of characters.\n",
    "- I suggest you split per page!\n",
    "- not sure how many characters exist per page, but using some domain knowledge about textbooks, we can tell that a topic is most likely captured in a single page\n",
    "hence, using the average character count per page of each chapter is a sound choice to start\n",
    "- so ideally you have 844 or sth chunks if you have 844 pages abt there\n",
    "- You think about it, like look at the textbook itself\n",
    "- and think \"if im a chunker, what's the best number of characters such that I can capture enouggh meaning without throwing away important detail\"\n",
    "- so probably just take the number of characters per page on average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7247f87-b7cb-4257-adc0-e054d33da901",
   "metadata": {},
   "source": [
    "chunk_overlap\n",
    "\n",
    "- overlap because you may be cutting off information prematurely without overlap\n",
    "- so if you have 20% char overlap, then expect 800*20% chunks\n",
    "- it's a decent value to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "50dad426-3f81-403f-97c5-dded10355256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the pdfs\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_list_of_documents(documents):\n",
    "    \"\"\"\n",
    "    input a list of documents as Document objects\n",
    "\n",
    "    output a list of chunks as Document objects\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 100, # using 20% is a good start\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dea8b3da-188d-4504-a5e8-4c424ace2559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = documents_dict['/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf']\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9a3a5b7e-1dc7-4316-8e74-6b79ab5237d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_documents(docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1ef94218-3177-404a-8ec4-c22c80c30103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/I748920/Desktop/llms-learning/pdf-chatbot-app/data/elements-of-statistical-learning-book/chap10.pdf', 'page': 1}, page_content='Training  SampleWeighted  SampleG(x) = sign[∑M\\nm=1αmGm(x)]\\nGM(x)\\nG3(x)\\nG2(x)\\nG1(x)Final Classifier\\nFIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ve r-\\nsions of the dataset, and then combined to produce a ﬁnal pred iction.\\nThe predictions from all of them are then combined through a w eighted\\nmajority vote to produce the ﬁnal prediction:\\nG(x) = sign(M∑\\nm=1αmGm(x))\\n. (10.1)\\nHereα1,α2,...,α Mare computed by the boosting algorithm, and weight')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b7316fd6-2f98-49a0-a203-359f52ff18f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 112.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5378"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for key in tqdm(documents_dict.keys()):\n",
    "    documents = documents_dict[key]\n",
    "    chunks = chunk_list_of_documents(documents=documents)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cf3b3-1b32-4155-8079-faa2bda5a951",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db49407-6c66-4320-ba85-d20bd9b6d9b4",
   "metadata": {},
   "source": [
    "all the chunks swee swee alr\n",
    "\n",
    "left with\n",
    "- indexing chunks use ollama-embeddings\n",
    "- create vectorstore using InMemoryVectorStore or Chroma\n",
    "- setup retriever -> retriever = vectorstore.as_retriever(search_type='similarity')\n",
    "- setup message history using InMemoryChatMessageHistory\n",
    "- setup prompts and rag chain\n",
    "- test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9713d80-1cad-46da-b9cb-f514fc424ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
