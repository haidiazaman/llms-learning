{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7a7dc2-78c9-45a7-adff-b4420525b10f",
   "metadata": {},
   "source": [
    "* https://docs.ragas.io/en/latest/concepts/testset_generation.html\n",
    "* https://github.com/explodinggradients/ragas/blob/main/src/ragas/testset/generator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc88720-2026-44ea-8d6c-f431052aa8f6",
   "metadata": {},
   "source": [
    "# try with openai LLM and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065be8bf-0708-47de-a954-9d60da985f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I748920/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_pdfs(file_paths):\n",
    "    \"\"\"\n",
    "    file_paths must end with .pdf\n",
    "    PyPDFLoader auto splits the pdf into pages, each page is 1 Document object split by page number\n",
    "    note that the splitting by page number is not perfect, the actual page number might be +/- 1-2pages.\n",
    "\n",
    "    returns a dict of key: file_path and value: list of document objects\n",
    "    \"\"\"\n",
    "    documents_dict = {}   \n",
    "    for f in tqdm(file_paths):\n",
    "        loader = PyPDFLoader(file_path = f)\n",
    "        documents = loader.load()\n",
    "        documents_dict[f] = documents\n",
    "    return documents_dict\n",
    "\n",
    "def chunk_list_of_documents(documents):\n",
    "    \"\"\"\n",
    "    input a list of documents as Document objects\n",
    "\n",
    "    output a list of chunks as Document objects\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 100, # using 20% is a good start\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db0d6ea-15eb-4edb-97f4-173fa50fd2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/I748920/Library/Python/3.9/lib/python/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# folder_path = \"/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book\"\n",
    "# os.path.exists(folder_path)\n",
    "# file_paths = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "documents_dict = load_pdfs(\n",
    "    file_paths=[\n",
    "        \"/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14603f6c-d923-450d-8377-596be1302815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for lst in list(documents_dict.values()):\n",
    "    documents.extend(lst)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb54c3b-2fd1-4fdc-ad64-56e4c9c4e2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0}, page_content='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792ec7be-eaf1-40e7-97d1-a12a4e9a8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']+f\" - page: {document.metadata['page']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2518e4-8eb4-41de-9a1d-27b09b983c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 0'}, page_content='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8990d7f9-63ff-46d8-9a9c-36f34c57054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f64965-b84f-45e6-b361-5aef756edb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestsetGenerator(generator_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False, seed=42)), critic_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False, seed=42)), embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x32c37a0a0>, docstore=InMemoryDocumentStore(splitter=<langchain_text_splitters.base.TokenTextSplitter object at 0x32c37a160>, nodes=[], node_embeddings_list=[], node_map={}, run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=(<class 'Exception'>,), log_tenacity=False, seed=42)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45879fba-d3c2-4c7b-8511-66f506a53a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.               \n",
      "Generating: 100%|██████████| 10/10 [08:30<00:00, 51.07s/it] \n"
     ]
    }
   ],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d239fa-b81b-43b6-8775-c520f07a11d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestDataset(test_data=[DataRow(question='What challenges arise when analyzing high-dimensional data?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='The analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. In the latter case, there is not enough information in the relatively small number of samples to efficiently estimate the high-dimensional covariance matrix. More regularization leads to superior prediction performance.', evolution_type='simple', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What is the purpose of feature selection in high-dimensional data analysis?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='The purpose of feature selection in high-dimensional data analysis is to regularize the analysis and improve prediction performance by selecting the most relevant features based on scientific contextual knowledge.', evolution_type='simple', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What is the role of Nearest Shrunken Centroids in feature selection and regularization in high-dimensional data analysis?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='Nearest Shrunken Centroids is a method used for feature selection and regularization in high-dimensional data analysis. It achieves feature selection by assuming that the features are independent within each class, resulting in a diagonal within-class covariance matrix. This method helps in regularizing the analysis of high-dimensional data by shrinking the centroids towards zero, effectively reducing the impact of irrelevant features. By using Nearest Shrunken Centroids, it is possible to select the most important features and improve prediction performance in high-dimensional data analysis.', evolution_type='simple', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What is the role of a linear model in high-dimensional prediction problems?', contexts=['This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-'], ground_truth='The role of a linear model in high-dimensional prediction problems is to predict the outcome variable based on a linear combination of the features. In equation (18.1), the outcome variable Y is predicted as the sum of the products of the feature values Xj and their corresponding coefficients βj, plus an error term ε. The linear model allows for the estimation of the coefficients βj, which represent the relationship between the features and the outcome variable.', evolution_type='simple', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 0'}]), DataRow(question='What challenges arise when analyzing high-dimensional data?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='The analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. In the latter case, there is not enough information in the relatively small number of samples to efficiently estimate the high-dimensional covariance matrix. More regularization leads to superior prediction performance.', evolution_type='simple', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What regularization method is used in linear discriminant analysis when the number of features is greater than the number of samples and assumes the within-class covariance matrix is diagonal?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='The answer to given question is not present in context', evolution_type='reasoning', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What method achieves feature selection and regularization in high-dimensional data analysis by assuming independent features within each class?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='Nearest Shrunken Centroids', evolution_type='reasoning', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What is the role of regularized approaches in genomics and computational biology?', contexts=['This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-'], ground_truth='Regularized approaches often become the methods of choice in genomics and computational biology due to the high variance and overfitting that are major concerns in this setting.', evolution_type='multi_context', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 0'}]), DataRow(question='What is the purpose of feature selection in high-dimensional data analysis and its relation to regularization and prediction performance?', contexts=['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'], ground_truth='Feature selection in high-dimensional data analysis is important for regularization and prediction performance. When dealing with high-dimensional data, regularization is necessary to prevent overfitting and improve prediction performance. Feature selection helps to identify the most relevant features and reduce the dimensionality of the data, which can improve the efficiency and interpretability of the analysis. By selecting a subset of informative features, regularization techniques can effectively estimate the high-dimensional covariance matrix and improve prediction performance in cases where there is limited information available. Therefore, feature selection plays a crucial role in high-dimensional data analysis by enhancing regularization and prediction performance.', evolution_type='multi_context', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]), DataRow(question='What is the role of regularized approaches in genomics and computational biology?', contexts=['This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-'], ground_truth='Regularized approaches often become the methods of choice in genomics and computational biology due to the high variance and overfitting that are major concerns in this setting.', evolution_type='multi_context', metadata=[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 0'}])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab86df60-c7a4-48f9-852f-a174f8158b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What challenges arise when analyzing high-dime...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>The analysis of high-dimensional data requires...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of feature selection in hi...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>The purpose of feature selection in high-dimen...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the role of Nearest Shrunken Centroids...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>Nearest Shrunken Centroids is a method used fo...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of a linear model in high-dim...</td>\n",
       "      <td>[This is page 649\\nPrinter: Opaque this\\n18\\nH...</td>\n",
       "      <td>The role of a linear model in high-dimensional...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What challenges arise when analyzing high-dime...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>The analysis of high-dimensional data requires...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What regularization method is used in linear d...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What method achieves feature selection and reg...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>Nearest Shrunken Centroids</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the role of regularized approaches in ...</td>\n",
       "      <td>[This is page 649\\nPrinter: Opaque this\\n18\\nH...</td>\n",
       "      <td>Regularized approaches often become the method...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the purpose of feature selection in hi...</td>\n",
       "      <td>[18.2 Nearest Shrunken Centroids 651\\nlow bias...</td>\n",
       "      <td>Feature selection in high-dimensional data ana...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the role of regularized approaches in ...</td>\n",
       "      <td>[This is page 649\\nPrinter: Opaque this\\n18\\nH...</td>\n",
       "      <td>Regularized approaches often become the method...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/Users/i748920/Desktop/llms-learn...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What challenges arise when analyzing high-dime...   \n",
       "1  What is the purpose of feature selection in hi...   \n",
       "2  What is the role of Nearest Shrunken Centroids...   \n",
       "3  What is the role of a linear model in high-dim...   \n",
       "4  What challenges arise when analyzing high-dime...   \n",
       "5  What regularization method is used in linear d...   \n",
       "6  What method achieves feature selection and reg...   \n",
       "7  What is the role of regularized approaches in ...   \n",
       "8  What is the purpose of feature selection in hi...   \n",
       "9  What is the role of regularized approaches in ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "1  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "2  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "3  [This is page 649\\nPrinter: Opaque this\\n18\\nH...   \n",
       "4  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "5  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "6  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "7  [This is page 649\\nPrinter: Opaque this\\n18\\nH...   \n",
       "8  [18.2 Nearest Shrunken Centroids 651\\nlow bias...   \n",
       "9  [This is page 649\\nPrinter: Opaque this\\n18\\nH...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The analysis of high-dimensional data requires...         simple   \n",
       "1  The purpose of feature selection in high-dimen...         simple   \n",
       "2  Nearest Shrunken Centroids is a method used fo...         simple   \n",
       "3  The role of a linear model in high-dimensional...         simple   \n",
       "4  The analysis of high-dimensional data requires...         simple   \n",
       "5  The answer to given question is not present in...      reasoning   \n",
       "6                         Nearest Shrunken Centroids      reasoning   \n",
       "7  Regularized approaches often become the method...  multi_context   \n",
       "8  Feature selection in high-dimensional data ana...  multi_context   \n",
       "9  Regularized approaches often become the method...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "1  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "2  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "3  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "4  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "5  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "6  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "7  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "8  [{'source': '/Users/i748920/Desktop/llms-learn...          True  \n",
       "9  [{'source': '/Users/i748920/Desktop/llms-learn...          True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49b13e52-8d82-4915-9820-24010751b3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'contexts', 'ground_truth', 'evolution_type', 'metadata',\n",
       "       'episode_done'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ee70d3-97eb-46fa-9ddf-4b1bd6aeac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What challenges arise when analyzing high-dimensional data?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffff0ed9-3195-4564-a116-55c93d76f837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f0b418c-1108-469c-94b4-538c9e9227cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. In the latter case, there is not enough information in the relatively small number of samples to efficiently estimate the high-dimensional covariance matrix. More regularization leads to superior prediction performance.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b9798cc-ad1a-4fc3-964a-5933a06b211d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb87968d-0d50-4ff5-b2c5-c14fc0556f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf',\n",
       "  'page': 2,\n",
       "  'filename': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-chap18-elements-of-statistical-learning-book/chap18 copy.pdf - page: 2'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f67f9a4-0ba3-4add-afb7-3cdd25393ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bea4a95-6030-4848-adc0-1feabb405062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What challenges arise when analyzing high-dimensional data?\n",
      "The analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. In the latter case, there is not enough information in the relatively small number of samples to efficiently estimate the high-dimensional covariance matrix. More regularization leads to superior prediction performance.\n",
      "simple\n",
      "1\n",
      "\n",
      "\n",
      "What is the purpose of feature selection in high-dimensional data analysis?\n",
      "The purpose of feature selection in high-dimensional data analysis is to regularize the analysis and improve prediction performance by selecting the most relevant features based on scientific contextual knowledge.\n",
      "simple\n",
      "1\n",
      "\n",
      "\n",
      "What is the role of Nearest Shrunken Centroids in feature selection and regularization in high-dimensional data analysis?\n",
      "Nearest Shrunken Centroids is a method used for feature selection and regularization in high-dimensional data analysis. It achieves feature selection by assuming that the features are independent within each class, resulting in a diagonal within-class covariance matrix. This method helps in regularizing the analysis of high-dimensional data by shrinking the centroids towards zero, effectively reducing the impact of irrelevant features. By using Nearest Shrunken Centroids, it is possible to select the most important features and improve prediction performance in high-dimensional data analysis.\n",
      "simple\n",
      "1\n",
      "\n",
      "\n",
      "What is the role of a linear model in high-dimensional prediction problems?\n",
      "The role of a linear model in high-dimensional prediction problems is to predict the outcome variable based on a linear combination of the features. In equation (18.1), the outcome variable Y is predicted as the sum of the products of the feature values Xj and their corresponding coefficients βj, plus an error term ε. The linear model allows for the estimation of the coefficients βj, which represent the relationship between the features and the outcome variable.\n",
      "simple\n",
      "1\n",
      "\n",
      "\n",
      "What challenges arise when analyzing high-dimensional data?\n",
      "The analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. In the latter case, there is not enough information in the relatively small number of samples to efficiently estimate the high-dimensional covariance matrix. More regularization leads to superior prediction performance.\n",
      "simple\n",
      "1\n",
      "\n",
      "\n",
      "What regularization method is used in linear discriminant analysis when the number of features is greater than the number of samples and assumes the within-class covariance matrix is diagonal?\n",
      "The answer to given question is not present in context\n",
      "reasoning\n",
      "1\n",
      "\n",
      "\n",
      "What method achieves feature selection and regularization in high-dimensional data analysis by assuming independent features within each class?\n",
      "Nearest Shrunken Centroids\n",
      "reasoning\n",
      "1\n",
      "\n",
      "\n",
      "What is the role of regularized approaches in genomics and computational biology?\n",
      "Regularized approaches often become the methods of choice in genomics and computational biology due to the high variance and overfitting that are major concerns in this setting.\n",
      "multi_context\n",
      "1\n",
      "\n",
      "\n",
      "What is the purpose of feature selection in high-dimensional data analysis and its relation to regularization and prediction performance?\n",
      "Feature selection in high-dimensional data analysis is important for regularization and prediction performance. When dealing with high-dimensional data, regularization is necessary to prevent overfitting and improve prediction performance. Feature selection helps to identify the most relevant features and reduce the dimensionality of the data, which can improve the efficiency and interpretability of the analysis. By selecting a subset of informative features, regularization techniques can effectively estimate the high-dimensional covariance matrix and improve prediction performance in cases where there is limited information available. Therefore, feature selection plays a crucial role in high-dimensional data analysis by enhancing regularization and prediction performance.\n",
      "multi_context\n",
      "1\n",
      "\n",
      "\n",
      "What is the role of regularized approaches in genomics and computational biology?\n",
      "Regularized approaches often become the methods of choice in genomics and computational biology due to the high variance and overfitting that are major concerns in this setting.\n",
      "multi_context\n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind,row in test_df.iterrows():\n",
    "    print(row[\"question\"])\n",
    "    print(row[\"ground_truth\"])\n",
    "    print(row[\"evolution_type\"])\n",
    "    print(len(row[\"contexts\"]))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0edc5852-6d78-46b5-adf3-dc42d0d791d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"sample_ragas_test_set.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f139731-aee1-4fc6-bbb7-b155f1d1825a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
